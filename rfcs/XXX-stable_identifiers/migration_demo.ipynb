{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94ddc27a",
   "metadata": {},
   "source": [
    "# ID Minter Migration Demo\n",
    "\n",
    "This notebook demonstrates the complete migration process for the stable identifiers proposal, including:\n",
    "\n",
    "1. **Current schema** - The existing 1:1 mapping between source identifiers and canonical IDs\n",
    "2. **Sample data** - Loading data representing the current state of the catalogue pipeline\n",
    "3. **Data migration** - Steps to migrate to the new schema with separate `canonical_ids` and `identifiers` tables\n",
    "4. **ID pre-generation** - How to maintain a pool of free IDs for efficient minting\n",
    "5. **Batch operations** - Efficient batch lookup and combined lookup/mint workflow\n",
    "6. **Predecessor inheritance** - How new records inherit canonical IDs from predecessors\n",
    "7. **Alias discovery** - How to identify which source identifiers share a canonical ID\n",
    "8. **New record minting** - Minting brand new records from the pre-generated pool\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Docker installed\n",
    "- [uv](https://docs.astral.sh/uv/) installed\n",
    "\n",
    "## Setup\n",
    "\n",
    "```bash\n",
    "cd /Users/kennyr/workspace/docs/rfcs/XXX-stable_identifiers\n",
    "\n",
    "# Install dependencies and create virtual environment\n",
    "uv sync\n",
    "\n",
    "# Start the MySQL container\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "Then select the `.venv` Python interpreter for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5cee4569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Connected to MySQL successfully\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Optional, Tuple\n",
    "import time\n",
    "\n",
    "from id_minter import generate_canonical_id\n",
    "\n",
    "# Database connection settings\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'port': 3306,\n",
    "    'user': 'root',\n",
    "    'password': 'rootpassword',\n",
    "    'database': 'id_minter'\n",
    "}\n",
    "\n",
    "def get_connection():\n",
    "    \"\"\"Get a database connection.\"\"\"\n",
    "    return pymysql.connect(**DB_CONFIG, cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "def execute_query(query: str, params: tuple = None, fetch: bool = False):\n",
    "    \"\"\"Execute a query and optionally fetch results.\"\"\"\n",
    "    conn = get_connection()\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(query, params)\n",
    "            if fetch:\n",
    "                return cursor.fetchall()\n",
    "            conn.commit()\n",
    "            return cursor.rowcount\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    conn = get_connection()\n",
    "    conn.close()\n",
    "    print(\"✓ Connected to MySQL successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Connection failed: {e}\")\n",
    "    print(\"\\nMake sure docker-compose is running:\")\n",
    "    print(\"  cd /Users/kennyr/workspace/docs/rfcs/XXX-stable_identifiers\")\n",
    "    print(\"  docker-compose up -d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a829dbc",
   "metadata": {},
   "source": [
    "## Step 1: Create the Current Schema\n",
    "\n",
    "This is the existing schema with a 1:1 relationship between canonical IDs and source identifiers, enforced by the primary key on `CanonicalId`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e75c95e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Current schema created\n",
      "\n",
      "Current 'identifiers' table structure:\n",
      "  CanonicalId          varchar(255)         PRI\n",
      "  OntologyType         varchar(255)         MUL\n",
      "  SourceId             varchar(255)         \n",
      "  SourceSystem         varchar(255)         \n"
     ]
    }
   ],
   "source": [
    "# Drop existing tables if they exist (for demo reset)\n",
    "execute_query(\"DROP TABLE IF EXISTS identifiers\")\n",
    "execute_query(\"DROP TABLE IF EXISTS identifiers_old\")\n",
    "execute_query(\"DROP TABLE IF EXISTS canonical_ids\")\n",
    "\n",
    "# Create the current (legacy) schema\n",
    "current_schema = \"\"\"\n",
    "CREATE TABLE identifiers (\n",
    "    CanonicalId VARCHAR(255) NOT NULL,\n",
    "    OntologyType VARCHAR(255) NOT NULL,\n",
    "    SourceId VARCHAR(255) NOT NULL,\n",
    "    SourceSystem VARCHAR(255) NOT NULL,\n",
    "    PRIMARY KEY (CanonicalId),\n",
    "    UNIQUE KEY UniqueFromSource (OntologyType, SourceSystem, SourceId)\n",
    ") ENGINE=InnoDB DEFAULT CHARSET=latin1\n",
    "\"\"\"\n",
    "\n",
    "execute_query(current_schema)\n",
    "print(\"✓ Current schema created\")\n",
    "\n",
    "# Verify the schema\n",
    "result = execute_query(\"DESCRIBE identifiers\", fetch=True)\n",
    "print(\"\\nCurrent 'identifiers' table structure:\")\n",
    "for row in result:\n",
    "    print(f\"  {row['Field']:20} {row['Type']:20} {row['Key']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1704e35",
   "metadata": {},
   "source": [
    "## Step 2: Load Sample Data\n",
    "\n",
    "Load the sample identifiers from the CSV file representing the current state of the catalogue pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "605781f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 10000 records from CSV\n",
      "\n",
      "Records by source system:\n",
      "  mets-image                       7781 records\n",
      "  sierra-system-number             1613 records\n",
      "  miro-image-number                 136 records\n",
      "  label-derived                     103 records\n",
      "  calm-record-id                     80 records\n",
      "  mets                               67 records\n",
      "  lc-names                           66 records\n",
      "  ebsco-alt-lookup                   50 records\n",
      "  calm-ref-no                        48 records\n",
      "  library-of-congress-names          20 records\n",
      "  lc-subjects                        17 records\n",
      "  nlm-mesh                           12 records\n",
      "  medical-subject-headings            3 records\n",
      "  tei-manuscript-id                   2 records\n",
      "  library-of-congress-subject-headings      2 records\n",
      "\n",
      "Sample records:\n",
      "  gbum7y2b <- sierra-system-number/1890040\n",
      "  be99823c <- mets-image/b28065037/FILE_0175_OBJECTS\n",
      "  thhp9d2t <- mets-image/b22372210/FILE_0045_OBJECTS\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Load sample data from CSV\n",
    "csv_path = '/Users/kennyr/workspace/docs/rfcs/XXX-stable_identifiers/identifiers_sample.csv'\n",
    "\n",
    "sample_data = []\n",
    "with open(csv_path, 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        sample_data.append({\n",
    "            'canonical_id': row['CanonicalId'],\n",
    "            'ontology_type': row['OntologyType'],\n",
    "            'source_system': row['SourceSystem'],\n",
    "            'source_id': row['SourceId']\n",
    "        })\n",
    "\n",
    "print(f\"✓ Loaded {len(sample_data)} records from CSV\")\n",
    "\n",
    "# Count by source system\n",
    "source_counts = {}\n",
    "for record in sample_data:\n",
    "    system = record['source_system']\n",
    "    source_counts[system] = source_counts.get(system, 0) + 1\n",
    "\n",
    "print(\"\\nRecords by source system:\")\n",
    "for system, count in sorted(source_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {system:30} {count:6} records\")\n",
    "\n",
    "print(f\"\\nSample records:\")\n",
    "for record in sample_data[:3]:\n",
    "    print(f\"  {record['canonical_id']} <- {record['source_system']}/{record['source_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ff3423f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Inserted 10000 records into current schema\n",
      "\n",
      "Records by source system:\n",
      "  mets-image                       7781 records\n",
      "  sierra-system-number             1613 records\n",
      "  miro-image-number                 136 records\n",
      "  label-derived                     103 records\n",
      "  calm-record-id                     80 records\n",
      "  mets                               67 records\n",
      "  lc-names                           66 records\n",
      "  ebsco-alt-lookup                   50 records\n",
      "  calm-ref-no                        48 records\n",
      "  library-of-congress-names          20 records\n",
      "  lc-subjects                        17 records\n",
      "  nlm-mesh                           12 records\n",
      "  medical-subject-headings            3 records\n",
      "  library-of-congress-subject-headings      2 records\n",
      "  tei-manuscript-id                   2 records\n"
     ]
    }
   ],
   "source": [
    "# Insert sample data into current schema\n",
    "conn = get_connection()\n",
    "cursor = conn.cursor()\n",
    "\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO identifiers (CanonicalId, OntologyType, SourceSystem, SourceId)\n",
    "VALUES (%s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "for record in sample_data:\n",
    "    cursor.execute(insert_query, (\n",
    "        record['canonical_id'],\n",
    "        record['ontology_type'],\n",
    "        record['source_system'],\n",
    "        record['source_id']\n",
    "    ))\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\"✓ Inserted {len(sample_data)} records into current schema\")\n",
    "\n",
    "# Verify counts\n",
    "result = execute_query(\"\"\"\n",
    "    SELECT SourceSystem, COUNT(*) as count \n",
    "    FROM identifiers \n",
    "    GROUP BY SourceSystem\n",
    "    ORDER BY count DESC\n",
    "\"\"\", fetch=True)\n",
    "\n",
    "print(\"\\nRecords by source system:\")\n",
    "for row in result:\n",
    "    print(f\"  {row['SourceSystem']:30} {row['count']:6} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bac4dba",
   "metadata": {},
   "source": [
    "## Step 3: Run the Migration\n",
    "\n",
    "Migrate to the new schema with:\n",
    "- `canonical_ids` table for ID registry and pre-generation\n",
    "- `identifiers` table allowing multiple source IDs per canonical ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "379ac0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating canonical_ids table...\n",
      "✓ canonical_ids table created\n",
      "\n",
      "'canonical_ids' table structure:\n",
      "  CanonicalId     varchar(8)                     PRI\n",
      "  Status          enum('free','assigned')        MUL\n",
      "  CreatedAt       timestamp                      \n"
     ]
    }
   ],
   "source": [
    "# Step 3a: Create the new canonical_ids table\n",
    "print(\"Creating canonical_ids table...\")\n",
    "\n",
    "canonical_ids_schema = \"\"\"\n",
    "CREATE TABLE canonical_ids (\n",
    "    CanonicalId VARCHAR(8) NOT NULL PRIMARY KEY,\n",
    "    Status ENUM('free', 'assigned') NOT NULL DEFAULT 'free',\n",
    "    CreatedAt TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    INDEX idx_free (Status, CanonicalId)\n",
    ") ENGINE=InnoDB DEFAULT CHARSET=latin1\n",
    "\"\"\"\n",
    "\n",
    "execute_query(canonical_ids_schema)\n",
    "print(\"✓ canonical_ids table created\")\n",
    "\n",
    "# Verify\n",
    "result = execute_query(\"DESCRIBE canonical_ids\", fetch=True)\n",
    "print(\"\\n'canonical_ids' table structure:\")\n",
    "for row in result:\n",
    "    print(f\"  {row['Field']:15} {row['Type']:30} {row['Key']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c40401d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating canonical_ids from existing data...\n",
      "✓ Inserted 10000 canonical IDs\n",
      "\n",
      "Total canonical IDs: 10000\n",
      "\n",
      "Sample canonical_ids records:\n",
      "  a22kzjax  assigned    2026-02-04 15:43:05\n",
      "  a22yfu6v  assigned    2026-02-04 15:43:05\n",
      "  a23v5jrt  assigned    2026-02-04 15:43:05\n",
      "  a24b67g9  assigned    2026-02-04 15:43:05\n",
      "  a26ap7qq  assigned    2026-02-04 15:43:05\n"
     ]
    }
   ],
   "source": [
    "# Step 3b: Populate canonical_ids from existing identifiers\n",
    "print(\"Populating canonical_ids from existing data...\")\n",
    "\n",
    "populate_query = \"\"\"\n",
    "INSERT INTO canonical_ids (CanonicalId, Status)\n",
    "SELECT DISTINCT CanonicalId, 'assigned' FROM identifiers\n",
    "\"\"\"\n",
    "\n",
    "rows_inserted = execute_query(populate_query)\n",
    "print(f\"✓ Inserted {rows_inserted} canonical IDs\")\n",
    "\n",
    "# Verify\n",
    "result = execute_query(\"SELECT COUNT(*) as count FROM canonical_ids\", fetch=True)\n",
    "print(f\"\\nTotal canonical IDs: {result[0]['count']}\")\n",
    "\n",
    "result = execute_query(\"SELECT * FROM canonical_ids LIMIT 5\", fetch=True)\n",
    "print(\"\\nSample canonical_ids records:\")\n",
    "for row in result:\n",
    "    print(f\"  {row['CanonicalId']}  {row['Status']:10}  {row['CreatedAt']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5abedfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new identifiers table...\n",
      "✓ identifiers_new table created\n",
      "\n",
      "'identifiers_new' table structure:\n",
      "  OntologyType    varchar(255)                   PRI\n",
      "  SourceSystem    varchar(255)                   PRI\n",
      "  SourceId        varchar(255)                   PRI\n",
      "  CanonicalId     varchar(8)                     MUL\n",
      "  CreatedAt       timestamp                      \n"
     ]
    }
   ],
   "source": [
    "# Step 3c: Create new identifiers table with updated schema\n",
    "print(\"Creating new identifiers table...\")\n",
    "\n",
    "new_identifiers_schema = \"\"\"\n",
    "CREATE TABLE identifiers_new (\n",
    "    OntologyType VARCHAR(255) NOT NULL,\n",
    "    SourceSystem VARCHAR(255) NOT NULL,\n",
    "    SourceId VARCHAR(255) NOT NULL,\n",
    "    CanonicalId VARCHAR(8) NOT NULL,\n",
    "    CreatedAt TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    PRIMARY KEY (OntologyType, SourceSystem, SourceId),\n",
    "    FOREIGN KEY (CanonicalId) REFERENCES canonical_ids(CanonicalId),\n",
    "    INDEX idx_canonical (CanonicalId)\n",
    ") ENGINE=InnoDB DEFAULT CHARSET=latin1\n",
    "\"\"\"\n",
    "\n",
    "execute_query(new_identifiers_schema)\n",
    "print(\"✓ identifiers_new table created\")\n",
    "\n",
    "# Verify\n",
    "result = execute_query(\"DESCRIBE identifiers_new\", fetch=True)\n",
    "print(\"\\n'identifiers_new' table structure:\")\n",
    "for row in result:\n",
    "    print(f\"  {row['Field']:15} {row['Type']:30} {row['Key']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "64447268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying data to new identifiers table...\n",
      "✓ Copied 10000 records\n",
      "\n",
      "Swapping tables...\n",
      "✓ Tables swapped\n",
      "\n",
      "Records in new 'identifiers' table: 10000\n",
      "\n",
      "Tables in database:\n",
      "  canonical_ids\n",
      "  identifiers\n",
      "  identifiers_old\n"
     ]
    }
   ],
   "source": [
    "# Step 3d: Copy data to new identifiers table\n",
    "print(\"Copying data to new identifiers table...\")\n",
    "\n",
    "copy_query = \"\"\"\n",
    "INSERT INTO identifiers_new (OntologyType, SourceSystem, SourceId, CanonicalId)\n",
    "SELECT OntologyType, SourceSystem, SourceId, CanonicalId FROM identifiers\n",
    "\"\"\"\n",
    "\n",
    "rows_copied = execute_query(copy_query)\n",
    "print(f\"✓ Copied {rows_copied} records\")\n",
    "\n",
    "# Step 3e: Swap tables (atomic rename)\n",
    "print(\"\\nSwapping tables...\")\n",
    "execute_query(\"RENAME TABLE identifiers TO identifiers_old, identifiers_new TO identifiers\")\n",
    "print(\"✓ Tables swapped\")\n",
    "\n",
    "# Verify final state\n",
    "result = execute_query(\"SELECT COUNT(*) as count FROM identifiers\", fetch=True)\n",
    "print(f\"\\nRecords in new 'identifiers' table: {result[0]['count']}\")\n",
    "\n",
    "result = execute_query(\"SHOW TABLES\", fetch=True)\n",
    "print(\"\\nTables in database:\")\n",
    "for row in result:\n",
    "    print(f\"  {list(row.values())[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e8812a",
   "metadata": {},
   "source": [
    "## Step 4: Pre-generate Free IDs\n",
    "\n",
    "Maintain a pool of pre-generated IDs to eliminate collision checking during minting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "56b79d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-generating free IDs...\n",
      "✓ Generated 100 new free IDs\n",
      "\n",
      "Canonical ID pool status:\n",
      "  free         100 IDs\n",
      "  assigned   10000 IDs\n"
     ]
    }
   ],
   "source": [
    "def pre_generate_ids(count: int) -> int:\n",
    "    \"\"\"Pre-generate a batch of free IDs.\"\"\"\n",
    "    conn = get_connection()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    generated = 0\n",
    "    attempts = 0\n",
    "    max_attempts = count * 2  # Allow for some collisions\n",
    "    \n",
    "    while generated < count and attempts < max_attempts:\n",
    "        new_id = generate_canonical_id()\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"INSERT IGNORE INTO canonical_ids (CanonicalId, Status) VALUES (%s, 'free')\",\n",
    "                (new_id,)\n",
    "            )\n",
    "            if cursor.rowcount > 0:\n",
    "                generated += 1\n",
    "        except Exception as e:\n",
    "            pass  # Collision or other error, try again\n",
    "        attempts += 1\n",
    "    \n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    return generated\n",
    "\n",
    "# Pre-generate 100 free IDs\n",
    "print(\"Pre-generating free IDs...\")\n",
    "generated = pre_generate_ids(100)\n",
    "print(f\"✓ Generated {generated} new free IDs\")\n",
    "\n",
    "# Check pool status\n",
    "result = execute_query(\"\"\"\n",
    "    SELECT Status, COUNT(*) as count \n",
    "    FROM canonical_ids \n",
    "    GROUP BY Status\n",
    "\"\"\", fetch=True)\n",
    "\n",
    "print(\"\\nCanonical ID pool status:\")\n",
    "for row in result:\n",
    "    print(f\"  {row['Status']:10} {row['count']:5} IDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6e9d13",
   "metadata": {},
   "source": [
    "## Step 5a: Test Batch Lookup\n",
    "\n",
    "The ID Minter supports batch lookup for efficient processing. A single query can look up many source identifiers across **mixed ontology types** (Works, Images, etc.) — returning only those that exist.\n",
    "\n",
    "This is the optimized hot path: most records processed already have canonical IDs. Missing IDs are then processed individually via `mint_id()` which handles predecessor inheritance and pool claiming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "19387b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ID Minter initialized\n",
      "\n",
      "============================================================\n",
      "Testing batch lookup (mixed ontology types)...\n",
      "============================================================\n",
      "\n",
      "Looking up 4 source IDs (mixed ontology types)...\n",
      "Found 2 existing canonical IDs:\n",
      "  Work/sierra-system-number/1001768 -> nvkdnjxp\n",
      "  Work/sierra-system-number/1007167 -> swbrj79k\n",
      "\n",
      "Not found (would need minting): 2\n",
      "  Image/sierra-system-number/1007828\n",
      "  Work/axiell-collections-id/DOES-NOT-EXIST\n"
     ]
    }
   ],
   "source": [
    "# Reload the module to pick up changes\n",
    "import importlib\n",
    "import id_minter\n",
    "importlib.reload(id_minter)\n",
    "from id_minter import IDMinter\n",
    "\n",
    "# Instantiate the minter with our connection factory\n",
    "minter = IDMinter(get_connection)\n",
    "print(\"✓ ID Minter initialized\")\n",
    "\n",
    "# Test batch lookup with a mix of existing and non-existing IDs\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing batch lookup (mixed ontology types)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get some existing Sierra IDs to test with\n",
    "existing_sierra = execute_query(\"\"\"\n",
    "    SELECT SourceId FROM identifiers \n",
    "    WHERE SourceSystem = 'sierra-system-number' AND OntologyType = 'Work'\n",
    "    LIMIT 3\n",
    "\"\"\", fetch=True)\n",
    "\n",
    "# Create test batch: mix of ontology types (Work + Image) and existing + non-existing\n",
    "test_batch = [\n",
    "    ('Work', 'sierra-system-number', existing_sierra[0]['SourceId']),\n",
    "    ('Work', 'sierra-system-number', existing_sierra[1]['SourceId']),\n",
    "    ('Image', 'sierra-system-number', existing_sierra[2]['SourceId']),  # Different ontology type (won't exist)\n",
    "    ('Work', 'axiell-collections-id', 'DOES-NOT-EXIST'),  # Non-existent source ID\n",
    "]\n",
    "\n",
    "print(f\"\\nLooking up {len(test_batch)} source IDs (mixed ontology types)...\")\n",
    "found = minter.lookup_ids(test_batch)\n",
    "\n",
    "print(f\"Found {len(found)} existing canonical IDs:\")\n",
    "for (ontology, system, sid), cid in found.items():\n",
    "    print(f\"  {ontology}/{system}/{sid} -> {cid}\")\n",
    "\n",
    "missing = [sid for sid in test_batch if sid not in found]\n",
    "print(f\"\\nNot found (would need minting): {len(missing)}\")\n",
    "for ontology, system, sid in missing:\n",
    "    print(f\"  {ontology}/{system}/{sid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f5cfca",
   "metadata": {},
   "source": [
    "## Step 5b: Test Batch Mint\n",
    "\n",
    "The `mint_ids()` method combines batch lookup with individual minting. It takes a list of `(source_id, predecessor_or_none)` tuples:\n",
    "\n",
    "```python\n",
    "results = minter.mint_ids([\n",
    "    (('Work', 'axiell', 'AC-123'), ('Work', 'sierra', 'b1234')),  # with predecessor\n",
    "    (('Image', 'mets', 'xyz'), None),  # no predecessor\n",
    "])\n",
    "```\n",
    "\n",
    "This optimizes the common case (most IDs already exist) while handling predecessor relationships correctly — including predecessors with different ontology types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d4a2a2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Testing mint_ids() with new signature...\n",
      "============================================================\n",
      "\n",
      "Processing 4 requests:\n",
      "  1. Work/sierra-system-number/1001768\n",
      "  2. Work/sierra-system-number/1007167\n",
      "  3. Work/axiell-collections-id/AC-BATCH-399846 <- Work/sierra-system-number/1001768\n",
      "  4. Work/axiell-collections-id/AC-BRAND-NEW-687806\n",
      "\n",
      "Results:\n",
      "  Work/sierra-system-number/1001768 -> nvkdnjxp (✓ found)\n",
      "  Work/sierra-system-number/1007167 -> swbrj79k (✓ found)\n",
      "  Work/axiell-collections-id/AC-BATCH-399846 -> nvkdnjxp (✓ inherited)\n",
      "  Work/axiell-collections-id/AC-BRAND-NEW-687806 -> bawutxdn (✓ minted)\n"
     ]
    }
   ],
   "source": [
    "# Reload the module to pick up changes\n",
    "importlib.reload(id_minter)\n",
    "from id_minter import IDMinter\n",
    "minter = IDMinter(get_connection)\n",
    "\n",
    "# Test mint_ids() - batch lookup + individual minting for missing IDs\n",
    "print(\"=\"*60)\n",
    "print(\"Testing mint_ids() with new signature...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get some existing Sierra IDs\n",
    "existing_works = execute_query(\"\"\"\n",
    "    SELECT SourceId, CanonicalId FROM identifiers \n",
    "    WHERE SourceSystem = 'sierra-system-number' AND OntologyType = 'Work'\n",
    "    LIMIT 2\n",
    "\"\"\", fetch=True)\n",
    "\n",
    "# Create requests as (source_id, predecessor_or_none) tuples:\n",
    "# - 2 existing Work IDs (should be found via batch lookup)\n",
    "# - 1 new Axiell ID with predecessor (should inherit canonical ID)\n",
    "# - 1 brand new ID (should claim from pool)\n",
    "requests = [\n",
    "    (('Work', 'sierra-system-number', existing_works[0]['SourceId']), None),  # Exists\n",
    "    (('Work', 'sierra-system-number', existing_works[1]['SourceId']), None),  # Exists\n",
    "    (('Work', 'axiell-collections-id', f'AC-BATCH-{random.randint(100000, 999999)}'), \n",
    "     ('Work', 'sierra-system-number', existing_works[0]['SourceId'])),  # New with predecessor\n",
    "    (('Work', 'axiell-collections-id', f'AC-BRAND-NEW-{random.randint(100000, 999999)}'), None),  # Brand new\n",
    "]\n",
    "\n",
    "print(f\"\\nProcessing {len(requests)} requests:\")\n",
    "for i, (source_id, predecessor) in enumerate(requests):\n",
    "    ont, sys, sid = source_id\n",
    "    pred_str = f\" <- {predecessor[0]}/{predecessor[1]}/{predecessor[2]}\" if predecessor else \"\"\n",
    "    print(f\"  {i+1}. {ont}/{sys}/{sid}{pred_str}\")\n",
    "\n",
    "# Call mint_ids with new signature\n",
    "results = minter.mint_ids(requests)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "for (ont, sys, sid), cid in results.items():\n",
    "    # Check if this was an existing ID or newly minted\n",
    "    was_existing = any(\n",
    "        sys == 'sierra-system-number' and sid == w['SourceId'] \n",
    "        for w in existing_works\n",
    "    )\n",
    "    status = \"✓ found\" if was_existing else \"✓ minted\"\n",
    "    \n",
    "    # Check predecessor inheritance\n",
    "    pred = next((r[1] for r in requests if r[0] == (ont, sys, sid) and r[1]), None)\n",
    "    if pred:\n",
    "        expected_cid = existing_works[0]['CanonicalId']\n",
    "        if cid == expected_cid:\n",
    "            status = \"✓ inherited\"\n",
    "        else:\n",
    "            status = \"✗ wrong ID!\"\n",
    "    \n",
    "    print(f\"  {ont}/{sys}/{sid} -> {cid} ({status})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555e044b",
   "metadata": {},
   "source": [
    "## Step 6: Demonstrate Predecessor Inheritance\n",
    "\n",
    "Simulate the migration by creating new Axiell Collections records that inherit canonical IDs from existing Sierra records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4b5846c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing Sierra Work records that will be migrated:\n",
      "\n",
      "  sierra-system-number/1001768 -> nvkdnjxp\n",
      "  sierra-system-number/1007167 -> swbrj79k\n",
      "  sierra-system-number/1007828 -> uxma4wqh\n",
      "  sierra-system-number/1009992 -> w67tu2mw\n",
      "  sierra-system-number/1011469 -> cz3nmzg2\n",
      "\n",
      "============================================================\n",
      "Simulating migration to Axiell Collections...\n",
      "============================================================\n",
      "\n",
      "\n",
      "Migrating record 1:\n",
      "  New Axiell ID: axiell-collections-id/AC-561973\n",
      "  Predecessor: Work/sierra-system-number/1001768\n",
      "  Result: Same canonical ID preserved! ✓\n",
      "\n",
      "Migrating record 2:\n",
      "  New Axiell ID: axiell-collections-id/AC-102107\n",
      "  Predecessor: Work/sierra-system-number/1007167\n",
      "  Result: Same canonical ID preserved! ✓\n",
      "\n",
      "Migrating record 3:\n",
      "  New Axiell ID: axiell-collections-id/AC-470629\n",
      "  Predecessor: Work/sierra-system-number/1007828\n",
      "  Result: Same canonical ID preserved! ✓\n",
      "\n",
      "Migrating record 4:\n",
      "  New Axiell ID: axiell-collections-id/AC-501598\n",
      "  Predecessor: Work/sierra-system-number/1009992\n",
      "  Result: Same canonical ID preserved! ✓\n",
      "\n",
      "Migrating record 5:\n",
      "  New Axiell ID: axiell-collections-id/AC-373599\n",
      "  Predecessor: Work/sierra-system-number/1011469\n",
      "  Result: Same canonical ID preserved! ✓\n"
     ]
    }
   ],
   "source": [
    "# Get some existing Sierra Work records to use as predecessors\n",
    "sierra_records = execute_query(\"\"\"\n",
    "    SELECT SourceId, CanonicalId FROM identifiers \n",
    "    WHERE SourceSystem = 'sierra-system-number' AND OntologyType = 'Work'\n",
    "    LIMIT 5\n",
    "\"\"\", fetch=True)\n",
    "\n",
    "print(\"Existing Sierra Work records that will be migrated:\\n\")\n",
    "for rec in sierra_records:\n",
    "    print(f\"  sierra-system-number/{rec['SourceId']} -> {rec['CanonicalId']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Simulating migration to Axiell Collections...\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Simulate Axiell Collections records referencing Sierra predecessors\n",
    "for i, sierra_rec in enumerate(sierra_records):\n",
    "    axiell_id = f\"AC-{random.randint(100000, 999999)}\"\n",
    "    \n",
    "    print(f\"\\nMigrating record {i+1}:\")\n",
    "    print(f\"  New Axiell ID: axiell-collections-id/{axiell_id}\")\n",
    "    print(f\"  Predecessor: Work/sierra-system-number/{sierra_rec['SourceId']}\")\n",
    "    \n",
    "    canonical_id = minter.mint_id(\n",
    "        ontology_type='Work',\n",
    "        source_system='axiell-collections-id',\n",
    "        source_id=axiell_id,\n",
    "        predecessor=('Work', 'sierra-system-number', sierra_rec['SourceId'])  # Full tuple\n",
    "    )\n",
    "    \n",
    "    # Verify it inherited the same canonical ID\n",
    "    assert canonical_id == sierra_rec['CanonicalId'], \"Should inherit predecessor's canonical ID!\"\n",
    "    print(f\"  Result: Same canonical ID preserved! ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc01fe60",
   "metadata": {},
   "source": [
    "## Step 7: Discover Aliases\n",
    "\n",
    "Query to find which source identifiers share a canonical ID and identify aliases by creation timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8bcd8c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 canonical IDs with multiple source identifiers:\n",
      "\n",
      "Canonical ID: cz3nmzg2\n",
      "------------------------------------------------------------\n",
      "  [Original] sierra-system-number/1011469\n",
      "           Created: 2026-02-04 15:43:15\n",
      "  [Alias   ] axiell-collections-id/AC-877528\n",
      "           Created: 2026-02-04 15:43:28\n",
      "\n",
      "Canonical ID: nvkdnjxp\n",
      "------------------------------------------------------------\n",
      "  [Original] sierra-system-number/1001768\n",
      "           Created: 2026-02-04 15:43:15\n",
      "  [Alias   ] axiell-collections-id/AC-886937\n",
      "           Created: 2026-02-04 15:43:28\n",
      "\n",
      "Canonical ID: swbrj79k\n",
      "------------------------------------------------------------\n",
      "  [Original] sierra-system-number/1007167\n",
      "           Created: 2026-02-04 15:43:15\n",
      "  [Alias   ] axiell-collections-id/AC-366490\n",
      "           Created: 2026-02-04 15:43:28\n",
      "\n",
      "Canonical ID: uxma4wqh\n",
      "------------------------------------------------------------\n",
      "  [Original] sierra-system-number/1007828\n",
      "           Created: 2026-02-04 15:43:15\n",
      "  [Alias   ] axiell-collections-id/AC-127829\n",
      "           Created: 2026-02-04 15:43:28\n",
      "\n",
      "Canonical ID: w67tu2mw\n",
      "------------------------------------------------------------\n",
      "  [Original] sierra-system-number/1009992\n",
      "           Created: 2026-02-04 15:43:15\n",
      "  [Alias   ] axiell-collections-id/AC-100029\n",
      "           Created: 2026-02-04 15:43:28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find canonical IDs with multiple source identifiers (aliases)\n",
    "alias_query = \"\"\"\n",
    "SELECT \n",
    "    CanonicalId, \n",
    "    COUNT(*) as source_count\n",
    "FROM identifiers \n",
    "GROUP BY CanonicalId \n",
    "HAVING COUNT(*) > 1\n",
    "ORDER BY source_count DESC\n",
    "\"\"\"\n",
    "\n",
    "aliased_ids = execute_query(alias_query, fetch=True)\n",
    "\n",
    "print(f\"Found {len(aliased_ids)} canonical IDs with multiple source identifiers:\\n\")\n",
    "\n",
    "# Show details for each aliased canonical ID\n",
    "for item in aliased_ids[:5]:  # Show first 5\n",
    "    canonical_id = item['CanonicalId']\n",
    "    \n",
    "    # Get all source identifiers for this canonical ID with alias status\n",
    "    details = execute_query(\"\"\"\n",
    "        SELECT \n",
    "            i.*,\n",
    "            CASE WHEN i.CreatedAt = earliest.MinCreatedAt THEN 'Original' ELSE 'Alias' END AS Status\n",
    "        FROM identifiers i\n",
    "        JOIN (\n",
    "            SELECT CanonicalId, MIN(CreatedAt) AS MinCreatedAt \n",
    "            FROM identifiers \n",
    "            GROUP BY CanonicalId\n",
    "        ) earliest ON i.CanonicalId = earliest.CanonicalId\n",
    "        WHERE i.CanonicalId = %s\n",
    "        ORDER BY i.CreatedAt\n",
    "    \"\"\", (canonical_id,), fetch=True)\n",
    "    \n",
    "    print(f\"Canonical ID: {canonical_id}\")\n",
    "    print(\"-\" * 60)\n",
    "    for d in details:\n",
    "        print(f\"  [{d['Status']:8}] {d['SourceSystem']}/{d['SourceId']}\")\n",
    "        print(f\"           Created: {d['CreatedAt']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da757a8",
   "metadata": {},
   "source": [
    "## Step 8: Mint New IDs (No Predecessor)\n",
    "\n",
    "Demonstrate minting brand new records that don't have predecessors - these claim IDs from the pre-generated pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d8b24eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pool status before minting:\n",
      "\n",
      "  free         100 IDs\n",
      "  assigned   10000 IDs\n",
      "\n",
      "============================================================\n",
      "Minting 5 brand new Axiell Collections records (no predecessor)\n",
      "============================================================\n",
      "\n",
      "\n",
      "Pool status after minting:\n",
      "\n",
      "  free          95 IDs\n",
      "  assigned   10005 IDs\n",
      "\n",
      "✓ Free IDs consumed: 5\n"
     ]
    }
   ],
   "source": [
    "# Check pool status before\n",
    "pool_before = execute_query(\"\"\"\n",
    "    SELECT Status, COUNT(*) as count \n",
    "    FROM canonical_ids \n",
    "    GROUP BY Status\n",
    "\"\"\", fetch=True)\n",
    "\n",
    "print(\"Pool status before minting:\\n\")\n",
    "for row in pool_before:\n",
    "    print(f\"  {row['Status']:10} {row['count']:5} IDs\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Minting 5 brand new Axiell Collections records (no predecessor)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "new_records = []\n",
    "for i in range(5):\n",
    "    axiell_id = f\"AC-NEW-{random.randint(100000, 999999)}\"\n",
    "    \n",
    "    canonical_id = minter.mint_id(\n",
    "        ontology_type='Work',\n",
    "        source_system='axiell-collections-id',\n",
    "        source_id=axiell_id\n",
    "        # No predecessor - this is a brand new record\n",
    "    )\n",
    "    \n",
    "    new_records.append({\n",
    "        'source_id': axiell_id,\n",
    "        'canonical_id': canonical_id\n",
    "    })\n",
    "\n",
    "# Check pool status after\n",
    "pool_after = execute_query(\"\"\"\n",
    "    SELECT Status, COUNT(*) as count \n",
    "    FROM canonical_ids \n",
    "    GROUP BY Status\n",
    "\"\"\", fetch=True)\n",
    "\n",
    "print(\"\\nPool status after minting:\\n\")\n",
    "for row in pool_after:\n",
    "    print(f\"  {row['Status']:10} {row['count']:5} IDs\")\n",
    "\n",
    "# The free count should have decreased by 5\n",
    "free_before = next(r['count'] for r in pool_before if r['Status'] == 'free')\n",
    "free_after = next(r['count'] for r in pool_after if r['Status'] == 'free')\n",
    "print(f\"\\n✓ Free IDs consumed: {free_before - free_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3fce18",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete migration process:\n",
    "\n",
    "1. **Current schema** - 1:1 mapping with PK on `CanonicalId`\n",
    "2. **Data loading** - Sample data from the catalogue pipeline\n",
    "3. **Migration** - Create `canonical_ids` table, restructure `identifiers` table\n",
    "4. **Pre-generation** - Pool of free IDs for efficient minting\n",
    "5. **Batch operations**:\n",
    "   - `lookup_ids()` - Single query to fetch multiple existing canonical IDs\n",
    "   - `mint_ids()` - Combined batch lookup + individual mint for missing IDs\n",
    "6. **Predecessor inheritance** - Migrated records inherit canonical IDs\n",
    "7. **Alias discovery** - Query to find linked source identifiers by `CreatedAt`\n",
    "8. **New record minting** - Claims from pre-generated pool\n",
    "\n",
    "### Key Benefits Demonstrated:\n",
    "- ✓ Stable canonical IDs across source system migrations\n",
    "- ✓ Batch lookup for efficient processing of existing records\n",
    "- ✓ Combined batch lookup + individual mint via `mint_ids()`\n",
    "- ✓ Single-record minting keeps predecessor handling simple\n",
    "- ✓ No collision retry loops (pre-generated IDs)  \n",
    "- ✓ Full provenance via `CreatedAt` timestamps\n",
    "- ✓ Support for future migrations (alias chaining)\n",
    "- ✓ `SKIP LOCKED` enables concurrent minting without contention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a736d37f",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Run this to stop and remove the Docker container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3fdaf2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Minter connection closed\n",
      "\u001b[?25l\u001b[0G[+] down 0/1\n",
      " \u001b[33m⠋\u001b[0m Container id-minter-mysql Stopping                                      \u001b[34m0.1s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[2A\u001b[0G[+] down 0/1\n",
      " \u001b[33m⠙\u001b[0m Container id-minter-mysql Stopping                                      \u001b[34m0.2s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[2A\u001b[0G[+] down 0/1\n",
      " \u001b[33m⠹\u001b[0m Container id-minter-mysql Stopping                                      \u001b[34m0.3s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[2A\u001b[0G[+] down 0/1\n",
      " \u001b[33m⠸\u001b[0m Container id-minter-mysql Stopping                                      \u001b[34m0.4s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[2A\u001b[0G[+] down 0/1\n",
      " \u001b[33m⠼\u001b[0m Container id-minter-mysql Stopping                                      \u001b[34m0.5s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[2A\u001b[0G[+] down 0/1\n",
      " \u001b[33m⠴\u001b[0m Container id-minter-mysql Stopping                                      \u001b[34m0.6s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[2A\u001b[0G[+] down 0/1\n",
      " \u001b[33m⠦\u001b[0m Container id-minter-mysql Stopping                                      \u001b[34m0.7s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[2A\u001b[0G[+] down 0/1\n",
      " \u001b[33m⠧\u001b[0m Container id-minter-mysql Stopping                                      \u001b[34m0.8s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[2A\u001b[0G[+] down 0/1\n",
      " \u001b[33m⠇\u001b[0m Container id-minter-mysql Stopping                                      \u001b[34m0.9s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[2A\u001b[0G[+] down 0/1\n",
      " \u001b[33m⠏\u001b[0m Container id-minter-mysql Stopping                                      \u001b[34m1.0s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[2A\u001b[0G[+] down 0/1\n",
      " \u001b[33m⠋\u001b[0m Container id-minter-mysql Stopping                                      \u001b[34m1.1s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[2A\u001b[0G[+] down 0/1\n",
      " \u001b[33m⠙\u001b[0m Container id-minter-mysql Stopping                                      \u001b[34m1.2s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[2A\u001b[0G[+] down 1/3\n",
      " \u001b[32m✔\u001b[0m Container id-minter-mysql                \u001b[32mRemoved\u001b[0m                        \u001b[34m1.3s \u001b[0m\n",
      " \u001b[33m⠋\u001b[0m Network xxx-stable_identifiers_default   Removing                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠋\u001b[0m Volume xxx-stable_identifiers_mysql_data Removing                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[4A\u001b[0G[+] down 2/3\n",
      " \u001b[32m✔\u001b[0m Container id-minter-mysql                \u001b[32mRemoved\u001b[0m                        \u001b[34m1.3s \u001b[0m\n",
      " \u001b[33m⠙\u001b[0m Network xxx-stable_identifiers_default   Removing                       \u001b[34m0.1s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Volume xxx-stable_identifiers_mysql_data \u001b[32mRemoved\u001b[0m                        \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[4A\u001b[0G[+] down 3/3\n",
      " \u001b[32m✔\u001b[0m Container id-minter-mysql                \u001b[32mRemoved\u001b[0m                        \u001b[34m1.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Network xxx-stable_identifiers_default   \u001b[32mRemoved\u001b[0m                        \u001b[34m0.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Volume xxx-stable_identifiers_mysql_data \u001b[32mRemoved\u001b[0m                        \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h✓ Docker container stopped\n"
     ]
    }
   ],
   "source": [
    "# Close connection and stop Docker container\n",
    "if 'conn' in dir() and conn and conn.open:\n",
    "    conn.close()\n",
    "    print(\"✓ Database connection closed\")\n",
    "\n",
    "if 'minter' in dir() and minter.conn and minter.conn.open:\n",
    "    minter.conn.close()\n",
    "    print(\"✓ Minter connection closed\")\n",
    "\n",
    "# Stop the container (use -v to also remove the volume)\n",
    "!docker compose down -v\n",
    "print(\"✓ Docker container stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable-identifiers-rfc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
