{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7768d5d7",
   "metadata": {},
   "source": [
    "# Schema Migration Demo\n",
    "\n",
    "This notebook demonstrates the database schema migration from the current 1:1 identifier mapping to the new schema supporting multiple source identifiers per canonical ID.\n",
    "\n",
    "## What this covers\n",
    "\n",
    "1. **Current schema** - The existing 1:1 mapping between source identifiers and canonical IDs\n",
    "2. **Sample data** - Loading data representing the current state of the catalogue pipeline\n",
    "3. **Migration steps** - Creating the new schema and migrating existing data\n",
    "4. **Verification** - Confirming the migration was successful with basic read/write tests\n",
    "\n",
    "For a detailed demonstration of the ID Minter functionality (batch operations, predecessor inheritance, race condition handling), see [id_minter_demo.ipynb](id_minter_demo.ipynb).\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Docker installed\n",
    "- [uv](https://docs.astral.sh/uv/) installed\n",
    "\n",
    "## Setup\n",
    "\n",
    "```bash\n",
    "cd /Users/kennyr/workspace/docs/rfcs/XXX-stable_identifiers\n",
    "\n",
    "# Install dependencies and create virtual environment\n",
    "uv sync\n",
    "\n",
    "# Start the MySQL container\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "Then select the `.venv` Python interpreter for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ac501f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Connected to MySQL successfully\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "import csv\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "from id_minter import generate_canonical_id\n",
    "\n",
    "# Database connection settings\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'port': 3306,\n",
    "    'user': 'root',\n",
    "    'password': 'rootpassword',\n",
    "    'database': 'id_minter'\n",
    "}\n",
    "\n",
    "def get_connection():\n",
    "    \"\"\"Get a database connection.\"\"\"\n",
    "    return pymysql.connect(**DB_CONFIG, cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "def execute_query(query: str, params: tuple = None, fetch: bool = False):\n",
    "    \"\"\"Execute a query and optionally fetch results.\"\"\"\n",
    "    conn = get_connection()\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(query, params)\n",
    "            if fetch:\n",
    "                return cursor.fetchall()\n",
    "            conn.commit()\n",
    "            return cursor.rowcount\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    conn = get_connection()\n",
    "    conn.close()\n",
    "    print(\"✓ Connected to MySQL successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Connection failed: {e}\")\n",
    "    print(\"\\nMake sure docker-compose is running:\")\n",
    "    print(\"  cd /Users/kennyr/workspace/docs/rfcs/XXX-stable_identifiers\")\n",
    "    print(\"  docker-compose up -d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c483e493",
   "metadata": {},
   "source": [
    "## Step 1: Create the Current (Legacy) Schema\n",
    "\n",
    "This is the existing schema with a 1:1 relationship between canonical IDs and source identifiers, enforced by the primary key on `CanonicalId`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b48f5abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Current (legacy) schema created\n",
      "\n",
      "Current 'identifiers' table structure:\n",
      "  CanonicalId          varchar(255)         PRI\n",
      "  OntologyType         varchar(255)         MUL\n",
      "  SourceId             varchar(255)         \n",
      "  SourceSystem         varchar(255)         \n"
     ]
    }
   ],
   "source": [
    "# Drop existing tables if they exist (for demo reset)\n",
    "execute_query(\"DROP TABLE IF EXISTS identifiers\")\n",
    "execute_query(\"DROP TABLE IF EXISTS identifiers_old\")\n",
    "execute_query(\"DROP TABLE IF EXISTS canonical_ids\")\n",
    "\n",
    "# Create the current (legacy) schema\n",
    "current_schema = \"\"\"\n",
    "CREATE TABLE identifiers (\n",
    "    CanonicalId VARCHAR(255) NOT NULL,\n",
    "    OntologyType VARCHAR(255) NOT NULL,\n",
    "    SourceId VARCHAR(255) NOT NULL,\n",
    "    SourceSystem VARCHAR(255) NOT NULL,\n",
    "    PRIMARY KEY (CanonicalId),\n",
    "    UNIQUE KEY UniqueFromSource (OntologyType, SourceSystem, SourceId)\n",
    ") ENGINE=InnoDB DEFAULT CHARSET=latin1\n",
    "\"\"\"\n",
    "\n",
    "execute_query(current_schema)\n",
    "print(\"✓ Current (legacy) schema created\")\n",
    "\n",
    "# Verify the schema\n",
    "result = execute_query(\"DESCRIBE identifiers\", fetch=True)\n",
    "print(\"\\nCurrent 'identifiers' table structure:\")\n",
    "for row in result:\n",
    "    print(f\"  {row['Field']:20} {row['Type']:20} {row['Key']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325e3b56",
   "metadata": {},
   "source": [
    "## Step 2: Load Sample Data\n",
    "\n",
    "Load the sample identifiers from the CSV file representing the current state of the catalogue pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80f6613f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 10000 records from CSV\n",
      "\n",
      "Records by source system:\n",
      "  mets-image                       7781 records\n",
      "  sierra-system-number             1613 records\n",
      "  miro-image-number                 136 records\n",
      "  label-derived                     103 records\n",
      "  calm-record-id                     80 records\n",
      "  mets                               67 records\n",
      "  lc-names                           66 records\n",
      "  ebsco-alt-lookup                   50 records\n",
      "  calm-ref-no                        48 records\n",
      "  library-of-congress-names          20 records\n",
      "  lc-subjects                        17 records\n",
      "  nlm-mesh                           12 records\n",
      "  medical-subject-headings            3 records\n",
      "  tei-manuscript-id                   2 records\n",
      "  library-of-congress-subject-headings      2 records\n",
      "\n",
      "Sample records:\n",
      "  gbum7y2b <- sierra-system-number/1890040\n",
      "  be99823c <- mets-image/b28065037/FILE_0175_OBJECTS\n",
      "  thhp9d2t <- mets-image/b22372210/FILE_0045_OBJECTS\n"
     ]
    }
   ],
   "source": [
    "# Load sample data from CSV\n",
    "csv_path = '/Users/kennyr/workspace/docs/rfcs/XXX-stable_identifiers/identifiers_sample.csv'\n",
    "\n",
    "sample_data = []\n",
    "with open(csv_path, 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        sample_data.append({\n",
    "            'canonical_id': row['CanonicalId'],\n",
    "            'ontology_type': row['OntologyType'],\n",
    "            'source_system': row['SourceSystem'],\n",
    "            'source_id': row['SourceId']\n",
    "        })\n",
    "\n",
    "print(f\"✓ Loaded {len(sample_data)} records from CSV\")\n",
    "\n",
    "# Count by source system\n",
    "source_counts = {}\n",
    "for record in sample_data:\n",
    "    system = record['source_system']\n",
    "    source_counts[system] = source_counts.get(system, 0) + 1\n",
    "\n",
    "print(\"\\nRecords by source system:\")\n",
    "for system, count in sorted(source_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {system:30} {count:6} records\")\n",
    "\n",
    "print(f\"\\nSample records:\")\n",
    "for record in sample_data[:3]:\n",
    "    print(f\"  {record['canonical_id']} <- {record['source_system']}/{record['source_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b065b7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Inserted 10000 records into legacy schema\n",
      "\n",
      "Records by source system in database:\n",
      "  mets-image                       7781 records\n",
      "  sierra-system-number             1613 records\n",
      "  miro-image-number                 136 records\n",
      "  label-derived                     103 records\n",
      "  calm-record-id                     80 records\n",
      "  mets                               67 records\n",
      "  lc-names                           66 records\n",
      "  ebsco-alt-lookup                   50 records\n",
      "  calm-ref-no                        48 records\n",
      "  library-of-congress-names          20 records\n",
      "  lc-subjects                        17 records\n",
      "  nlm-mesh                           12 records\n",
      "  medical-subject-headings            3 records\n",
      "  library-of-congress-subject-headings      2 records\n",
      "  tei-manuscript-id                   2 records\n"
     ]
    }
   ],
   "source": [
    "# Insert sample data into current schema\n",
    "conn = get_connection()\n",
    "cursor = conn.cursor()\n",
    "\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO identifiers (CanonicalId, OntologyType, SourceSystem, SourceId)\n",
    "VALUES (%s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "for record in sample_data:\n",
    "    cursor.execute(insert_query, (\n",
    "        record['canonical_id'],\n",
    "        record['ontology_type'],\n",
    "        record['source_system'],\n",
    "        record['source_id']\n",
    "    ))\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\"✓ Inserted {len(sample_data)} records into legacy schema\")\n",
    "\n",
    "# Verify counts\n",
    "result = execute_query(\"\"\"\n",
    "    SELECT SourceSystem, COUNT(*) as count \n",
    "    FROM identifiers \n",
    "    GROUP BY SourceSystem\n",
    "    ORDER BY count DESC\n",
    "\"\"\", fetch=True)\n",
    "\n",
    "print(\"\\nRecords by source system in database:\")\n",
    "for row in result:\n",
    "    print(f\"  {row['SourceSystem']:30} {row['count']:6} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d23d2d5",
   "metadata": {},
   "source": [
    "## Step 3: Run the Migration\n",
    "\n",
    "Migrate to the new schema with:\n",
    "- `canonical_ids` table for ID registry and pre-generation\n",
    "- `identifiers` table allowing multiple source IDs per canonical ID\n",
    "\n",
    "### Step 3a: Create the canonical_ids table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b957634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating canonical_ids table...\n",
      "✓ canonical_ids table created\n",
      "\n",
      "'canonical_ids' table structure:\n",
      "  CanonicalId     varchar(8)                     PRI\n",
      "  Status          enum('free','assigned')        MUL\n",
      "  CreatedAt       timestamp                      \n"
     ]
    }
   ],
   "source": [
    "print(\"Creating canonical_ids table...\")\n",
    "\n",
    "canonical_ids_schema = \"\"\"\n",
    "CREATE TABLE canonical_ids (\n",
    "    CanonicalId VARCHAR(8) NOT NULL PRIMARY KEY,\n",
    "    Status ENUM('free', 'assigned') NOT NULL DEFAULT 'free',\n",
    "    CreatedAt TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    INDEX idx_free (Status, CanonicalId)\n",
    ") ENGINE=InnoDB DEFAULT CHARSET=latin1\n",
    "\"\"\"\n",
    "\n",
    "execute_query(canonical_ids_schema)\n",
    "print(\"✓ canonical_ids table created\")\n",
    "\n",
    "# Verify\n",
    "result = execute_query(\"DESCRIBE canonical_ids\", fetch=True)\n",
    "print(\"\\n'canonical_ids' table structure:\")\n",
    "for row in result:\n",
    "    print(f\"  {row['Field']:15} {row['Type']:30} {row['Key']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f12ccb",
   "metadata": {},
   "source": [
    "### Step 3b: Populate canonical_ids from existing identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddf0e53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating canonical_ids from existing data...\n",
      "✓ Inserted 10000 canonical IDs\n",
      "\n",
      "Total canonical IDs: 10000\n",
      "\n",
      "Sample canonical_ids records:\n",
      "  a22kzjax  assigned    2026-02-05 09:22:42\n",
      "  a22yfu6v  assigned    2026-02-05 09:22:42\n",
      "  a23v5jrt  assigned    2026-02-05 09:22:42\n",
      "  a24b67g9  assigned    2026-02-05 09:22:42\n",
      "  a26ap7qq  assigned    2026-02-05 09:22:42\n"
     ]
    }
   ],
   "source": [
    "print(\"Populating canonical_ids from existing data...\")\n",
    "\n",
    "populate_query = \"\"\"\n",
    "INSERT INTO canonical_ids (CanonicalId, Status)\n",
    "SELECT DISTINCT CanonicalId, 'assigned' FROM identifiers\n",
    "\"\"\"\n",
    "\n",
    "rows_inserted = execute_query(populate_query)\n",
    "print(f\"✓ Inserted {rows_inserted} canonical IDs\")\n",
    "\n",
    "# Verify\n",
    "result = execute_query(\"SELECT COUNT(*) as count FROM canonical_ids\", fetch=True)\n",
    "print(f\"\\nTotal canonical IDs: {result[0]['count']}\")\n",
    "\n",
    "result = execute_query(\"SELECT * FROM canonical_ids LIMIT 5\", fetch=True)\n",
    "print(\"\\nSample canonical_ids records:\")\n",
    "for row in result:\n",
    "    print(f\"  {row['CanonicalId']}  {row['Status']:10}  {row['CreatedAt']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009ffa57",
   "metadata": {},
   "source": [
    "### Step 3c: Create new identifiers table with updated schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88df5b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new identifiers table...\n",
      "✓ identifiers_new table created\n",
      "\n",
      "'identifiers_new' table structure:\n",
      "  OntologyType    varchar(255)                   PRI\n",
      "  SourceSystem    varchar(255)                   PRI\n",
      "  SourceId        varchar(255)                   PRI\n",
      "  CanonicalId     varchar(8)                     MUL\n",
      "  CreatedAt       timestamp                      \n"
     ]
    }
   ],
   "source": [
    "print(\"Creating new identifiers table...\")\n",
    "\n",
    "new_identifiers_schema = \"\"\"\n",
    "CREATE TABLE identifiers_new (\n",
    "    OntologyType VARCHAR(255) NOT NULL,\n",
    "    SourceSystem VARCHAR(255) NOT NULL,\n",
    "    SourceId VARCHAR(255) NOT NULL,\n",
    "    CanonicalId VARCHAR(8) NOT NULL,\n",
    "    CreatedAt TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    PRIMARY KEY (OntologyType, SourceSystem, SourceId),\n",
    "    FOREIGN KEY (CanonicalId) REFERENCES canonical_ids(CanonicalId),\n",
    "    INDEX idx_canonical (CanonicalId)\n",
    ") ENGINE=InnoDB DEFAULT CHARSET=latin1\n",
    "\"\"\"\n",
    "\n",
    "execute_query(new_identifiers_schema)\n",
    "print(\"✓ identifiers_new table created\")\n",
    "\n",
    "# Verify\n",
    "result = execute_query(\"DESCRIBE identifiers_new\", fetch=True)\n",
    "print(\"\\n'identifiers_new' table structure:\")\n",
    "for row in result:\n",
    "    print(f\"  {row['Field']:15} {row['Type']:30} {row['Key']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c0abb1",
   "metadata": {},
   "source": [
    "### Step 3d: Copy data and swap tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2c64aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying data to new identifiers table...\n",
      "✓ Copied 10000 records\n",
      "\n",
      "Swapping tables (atomic rename)...\n",
      "✓ Tables swapped\n",
      "\n",
      "Records in new 'identifiers' table: 10000\n",
      "\n",
      "Tables in database:\n",
      "  canonical_ids\n",
      "  identifiers\n",
      "  identifiers_old\n"
     ]
    }
   ],
   "source": [
    "print(\"Copying data to new identifiers table...\")\n",
    "\n",
    "copy_query = \"\"\"\n",
    "INSERT INTO identifiers_new (OntologyType, SourceSystem, SourceId, CanonicalId)\n",
    "SELECT OntologyType, SourceSystem, SourceId, CanonicalId FROM identifiers\n",
    "\"\"\"\n",
    "\n",
    "rows_copied = execute_query(copy_query)\n",
    "print(f\"✓ Copied {rows_copied} records\")\n",
    "\n",
    "# Atomic table swap\n",
    "print(\"\\nSwapping tables (atomic rename)...\")\n",
    "execute_query(\"RENAME TABLE identifiers TO identifiers_old, identifiers_new TO identifiers\")\n",
    "print(\"✓ Tables swapped\")\n",
    "\n",
    "# Verify final state\n",
    "result = execute_query(\"SELECT COUNT(*) as count FROM identifiers\", fetch=True)\n",
    "print(f\"\\nRecords in new 'identifiers' table: {result[0]['count']}\")\n",
    "\n",
    "result = execute_query(\"SHOW TABLES\", fetch=True)\n",
    "print(\"\\nTables in database:\")\n",
    "for row in result:\n",
    "    print(f\"  {list(row.values())[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5126a39",
   "metadata": {},
   "source": [
    "## Step 4: Pre-generate Free IDs\n",
    "\n",
    "Maintain a pool of pre-generated IDs to eliminate collision checking during minting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f14c1746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-generating free IDs...\n",
      "✓ Generated 100 new free IDs\n",
      "\n",
      "Canonical ID pool status:\n",
      "  free         100 IDs\n",
      "  assigned   10000 IDs\n"
     ]
    }
   ],
   "source": [
    "def pre_generate_ids(count: int) -> int:\n",
    "    \"\"\"Pre-generate a batch of free IDs.\"\"\"\n",
    "    conn = get_connection()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    generated = 0\n",
    "    attempts = 0\n",
    "    max_attempts = count * 2  # Allow for some collisions\n",
    "    \n",
    "    while generated < count and attempts < max_attempts:\n",
    "        new_id = generate_canonical_id()\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"INSERT IGNORE INTO canonical_ids (CanonicalId, Status) VALUES (%s, 'free')\",\n",
    "                (new_id,)\n",
    "            )\n",
    "            if cursor.rowcount > 0:\n",
    "                generated += 1\n",
    "        except Exception as e:\n",
    "            pass  # Collision or other error, try again\n",
    "        attempts += 1\n",
    "    \n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    return generated\n",
    "\n",
    "# Pre-generate 100 free IDs\n",
    "print(\"Pre-generating free IDs...\")\n",
    "generated = pre_generate_ids(100)\n",
    "print(f\"✓ Generated {generated} new free IDs\")\n",
    "\n",
    "# Check pool status\n",
    "result = execute_query(\"\"\"\n",
    "    SELECT Status, COUNT(*) as count \n",
    "    FROM canonical_ids \n",
    "    GROUP BY Status\n",
    "\"\"\", fetch=True)\n",
    "\n",
    "print(\"\\nCanonical ID pool status:\")\n",
    "for row in result:\n",
    "    print(f\"  {row['Status']:10} {row['count']:5} IDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9425cd40",
   "metadata": {},
   "source": [
    "## Step 5: Verify Migration with ID Minter\n",
    "\n",
    "Confirm the migration was successful by testing basic read/write operations with the ID Minter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "588e7fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ID Minter initialized\n",
      "\n",
      "============================================================\n",
      "Testing batch lookup on migrated data...\n",
      "============================================================\n",
      "\n",
      "Looked up 3 IDs, found 3:\n",
      "  ✓ Item/sierra-system-number/i19854572 -> a22yfu6v\n",
      "  ✓ Item/sierra-system-number/1016923 -> a329mxe7\n",
      "  ✓ Work/sierra-system-number/b13439844 -> a45fug5q\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import id_minter\n",
    "importlib.reload(id_minter)\n",
    "from id_minter import IDMinter\n",
    "\n",
    "# Instantiate the minter\n",
    "minter = IDMinter(get_connection)\n",
    "print(\"✓ ID Minter initialized\")\n",
    "\n",
    "# Test batch lookup - verify we can read existing data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing batch lookup on migrated data...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get some existing Sierra IDs\n",
    "existing = execute_query(\"\"\"\n",
    "    SELECT OntologyType, SourceSystem, SourceId, CanonicalId \n",
    "    FROM identifiers \n",
    "    WHERE SourceSystem = 'sierra-system-number'\n",
    "    LIMIT 3\n",
    "\"\"\", fetch=True)\n",
    "\n",
    "lookup_ids = [(r['OntologyType'], r['SourceSystem'], r['SourceId']) for r in existing]\n",
    "found = minter.lookup_ids(lookup_ids)\n",
    "\n",
    "print(f\"\\nLooked up {len(lookup_ids)} IDs, found {len(found)}:\")\n",
    "for (ont, sys, sid), cid in found.items():\n",
    "    expected = next(r['CanonicalId'] for r in existing if r['SourceId'] == sid)\n",
    "    status = \"✓\" if cid == expected else \"✗\"\n",
    "    print(f\"  {status} {ont}/{sys}/{sid} -> {cid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cae1cb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Testing mint_ids on migrated schema...\n",
      "============================================================\n",
      "\n",
      "Minted new ID:\n",
      "  Work/test-system/MIGRATION-TEST-872008 -> a26hunyp\n",
      "\n",
      "Free IDs consumed: 1\n",
      "\n",
      "✓ Migration verified - read and write operations successful!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Test minting a new ID - verify we can write to the new schema\n",
    "print(\"=\"*60)\n",
    "print(\"Testing mint_ids on migrated schema...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get pool status before\n",
    "pool_before = execute_query(\"\"\"\n",
    "    SELECT Status, COUNT(*) as count FROM canonical_ids GROUP BY Status\n",
    "\"\"\", fetch=True)\n",
    "free_before = next(r['count'] for r in pool_before if r['Status'] == 'free')\n",
    "\n",
    "# Mint a new ID\n",
    "test_id = f\"MIGRATION-TEST-{random.randint(100000, 999999)}\"\n",
    "results = minter.mint_ids([\n",
    "    (('Work', 'test-system', test_id), None)\n",
    "])\n",
    "\n",
    "# Get pool status after\n",
    "pool_after = execute_query(\"\"\"\n",
    "    SELECT Status, COUNT(*) as count FROM canonical_ids GROUP BY Status\n",
    "\"\"\", fetch=True)\n",
    "free_after = next(r['count'] for r in pool_after if r['Status'] == 'free')\n",
    "\n",
    "print(f\"\\nMinted new ID:\")\n",
    "for (ont, sys, sid), cid in results.items():\n",
    "    print(f\"  {ont}/{sys}/{sid} -> {cid}\")\n",
    "\n",
    "print(f\"\\nFree IDs consumed: {free_before - free_after}\")\n",
    "print(\"\\n✓ Migration verified - read and write operations successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29b3672",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete schema migration:\n",
    "\n",
    "1. **Legacy schema created** - 1:1 mapping with PK on `CanonicalId`\n",
    "2. **Sample data loaded** - 10,000 records from the catalogue pipeline\n",
    "3. **Migration executed**:\n",
    "   - Created `canonical_ids` table for ID registry\n",
    "   - Populated with existing IDs (status: 'assigned')\n",
    "   - Created new `identifiers` table with composite PK\n",
    "   - Atomic table swap\n",
    "4. **Free IDs pre-generated** - Pool for efficient minting\n",
    "5. **Migration verified** - Read and write operations successful\n",
    "\n",
    "### Key Schema Changes\n",
    "\n",
    "| Aspect | Legacy | New |\n",
    "|--------|--------|-----|\n",
    "| Primary key | `CanonicalId` | `(OntologyType, SourceSystem, SourceId)` |\n",
    "| Mapping | 1:1 (source → canonical) | Many:1 (multiple sources → one canonical) |\n",
    "| ID generation | On-demand with collision retry | Pre-generated pool |\n",
    "| Timestamps | None | `CreatedAt` for provenance |\n",
    "\n",
    "For detailed ID Minter functionality (batch operations, predecessor inheritance, race conditions), see [id_minter_demo.ipynb](id_minter_demo.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4ce934",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Run this to stop and remove the Docker container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f10a10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Minter connection closed\n",
      "\u001b[?25l\u001b[0G[+] down 0/1\n",
      " \u001b[33m⠋\u001b[0m Container id-minter-mysql Stopping                                      \u001b[34m0.1s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[2A\u001b[0G[+] down 0/1\n",
      " \u001b[33m⠙\u001b[0m Container id-minter-mysql Stopping                                      \u001b[34m0.2s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[2A\u001b[0G[+] down 0/1\n",
      " \u001b[33m⠹\u001b[0m Container id-minter-mysql Stopping                                      \u001b[34m0.3s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[2A\u001b[0G[+] down 0/1\n",
      " \u001b[33m⠸\u001b[0m Container id-minter-mysql Stopping                                      \u001b[34m0.4s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[2A\u001b[0G[+] down 0/1\n",
      " \u001b[33m⠼\u001b[0m Container id-minter-mysql Stopping                                      \u001b[34m0.5s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[2A\u001b[0G[+] down 0/1\n",
      " \u001b[33m⠴\u001b[0m Container id-minter-mysql Stopping                                      \u001b[34m0.6s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[2A\u001b[0G[+] down 0/1\n",
      " \u001b[33m⠦\u001b[0m Container id-minter-mysql Stopping                                      \u001b[34m0.7s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[2A\u001b[0G[+] down 0/1\n",
      " \u001b[33m⠧\u001b[0m Container id-minter-mysql Stopping                                      \u001b[34m0.8s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[2A\u001b[0G[+] down 0/1\n",
      " \u001b[33m⠇\u001b[0m Container id-minter-mysql Stopping                                      \u001b[34m0.9s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[2A\u001b[0G[+] down 0/1\n",
      " \u001b[33m⠏\u001b[0m Container id-minter-mysql Stopping                                      \u001b[34m1.0s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[2A\u001b[0G[+] down 0/1\n",
      " \u001b[33m⠋\u001b[0m Container id-minter-mysql Stopping                                      \u001b[34m1.1s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[2A\u001b[0G[+] down 0/1\n",
      " \u001b[33m⠙\u001b[0m Container id-minter-mysql Stopping                                      \u001b[34m1.2s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[2A\u001b[0G[+] down 2/3\n",
      " \u001b[32m✔\u001b[0m Container id-minter-mysql                \u001b[32mRemoved\u001b[0m                        \u001b[34m1.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Volume xxx-stable_identifiers_mysql_data \u001b[32mRemoved\u001b[0m                        \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠋\u001b[0m Network xxx-stable_identifiers_default   Removing                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[4A\u001b[0G[+] down 2/3\n",
      " \u001b[32m✔\u001b[0m Container id-minter-mysql                \u001b[32mRemoved\u001b[0m                        \u001b[34m1.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Volume xxx-stable_identifiers_mysql_data \u001b[32mRemoved\u001b[0m                        \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠙\u001b[0m Network xxx-stable_identifiers_default   Removing                       \u001b[34m0.1s \u001b[0m\n",
      "\u001b[?25h\u001b[?25l\u001b[4A\u001b[0G[+] down 3/3\n",
      " \u001b[32m✔\u001b[0m Container id-minter-mysql                \u001b[32mRemoved\u001b[0m                        \u001b[34m1.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Volume xxx-stable_identifiers_mysql_data \u001b[32mRemoved\u001b[0m                        \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Network xxx-stable_identifiers_default   \u001b[32mRemoved\u001b[0m                        \u001b[34m0.2s \u001b[0m\n",
      "\u001b[?25h✓ Docker container stopped\n"
     ]
    }
   ],
   "source": [
    "# Close connections\n",
    "if 'minter' in dir() and minter.conn and minter.conn.open:\n",
    "    minter.conn.close()\n",
    "    print(\"✓ Minter connection closed\")\n",
    "\n",
    "# Stop the container (use -v to also remove the volume)\n",
    "!docker compose down -v\n",
    "print(\"✓ Docker container stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable-identifiers-rfc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
