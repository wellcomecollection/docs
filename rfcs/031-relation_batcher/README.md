# RFC 031: Relation Batcher

## Background

The `relation_embedder` was recently introduced into the pipeline to denormalise related works into a works data, so that a particular work in an archive retrieved from the API will contain it's children, ancestors and siblings.

## Problem

Currently the `relation_embedder` is not processing works at a rate that is sufficient, being orders of magnitude slower than other pipeline stages.

There has already been a number of PRs which have attempted to allieviate this by optimising the requests sent to Elasticsearch for indexing, retrieval and querying of relations:

* [#1007 Try to make the relation embedder more efficient](https://github.com/wellcomecollection/catalogue/pull/1007)
* [#1011 Don't index unnecessary fields in pipeline storage](https://github.com/wellcomecollection/catalogue/pull/1011)
* [#1017 Use scroll API and return works directly in the getAffectedWorks query](https://github.com/wellcomecollection/catalogue/pull/1017)
* [#1018 Cache all relations for an archive up front of denormalising](https://github.com/wellcomecollection/catalogue/pull/1018)

This work is definately useful (and with [#1011](https://github.com/wellcomecollection/catalogue/pull/1011) greatly improves pipeline storage indexing performance for other services too), but is unlikely on its own to make a significant enough improvement to throughput, due to more fundamental problems with the work performed by the `relation_embedder`.

The main issue is that for a single input to the `relation_embedder`, it can result in many works being denormalised. Given the following tree:

```
A
|-----
|    |
B    C
|    |-----
|    |    |
D    E    F
```

When a message is received for work `D` it will need denormalise 2 works, namely `B` and `D`: it needs to denormalise the work itself, and it's parent `B` due to it needing to update it's `children` field. When a message is received for work `A` however it will need to denormalise every work in the tree, due to all of the works containing `A` as an ancestor.

When building the `relation_embedder` the assumpion was made that many more works will be further down the tree and leaf nodes than being near the root, so the amount of work performed will be managable. However this assumpion doesn't really hold, with a combinatorially large amount of work being generated by nodes that are high in the tree of large archives.

There are also extreme cases where even though most of the works in an archive are leaf nodes, the archive is very flat. To take one real world example:

```
MS9225
|
|---
|  |
1  2
   |-------------
   |  |  |      |
   1  2  3 ... 3583
```

There are 3583 works here which are siblings of eachother, and when any one of these is received at the input it will result in all of their siblings and the two ancestors needing to be denormalised too. For an initial reindex that will result in denormalising 6,427,905 works (`1 + 2 + ... + 3585`), and in a reharvest where in the worst case all of the works in the archive are sent again it will result in 12,852,225 (`3585 * 3585`). In an ideal scenario, we would of course only denormalise each of the works in that archive one time only (i.e. 3,585 denormalisations), with anything above that being unnecessary work. Note this is just a single archive, and there will likely be other similar cases out of the 6349 archives currently in Calm.

## Proposed Solution

The proposed solution is to have a relation embedder pipeline made of 3 services: `router`, a `batcher` and 
the `relation_embedder`. 

### Router
The `router` takes a work id, looks it up in the `pipeline_storage` and checks if it's part of an 
archive (ie. has a `collectionPath` field). 

If it's not, it sends it straight to the `id_minter_works`. 

If it is, it sends the value of `collectionPath` to the `batcher`

### Batcher
The `batcher` is deployed as a single task and runs at intervals of fixed duration (say, 30  minutes). 
At every run, it reads all messages in the queue at that moment. 
Assuming a graph shape like this:
```
A
|-----
|    |
B    C
|    |-----
|    |    |
D    E    F
|
G
|
H
```

and assuming the queue contains `H`, `D`, `E` and `F` the `batcher` would:

- determine the set parent of each of the messages in the queue (ie. in the example `G`, `B`, `C`)
- filter out nodes which are descendents from other ones (i,e, `G` as it is a descendent of `B`) 
- send the remaining to the `relation_embedder` (`B` and `C` )

In the example above of this archive tree:
```
MS9225
|
|---
|  |
1  2
   |-------------
   |  |  |      |
   1  2  3 ... 3583
```

in the best case scenario where all works in this archive are in the queue at the same time and
the `batcher` sends only one work - `MS9225` - to the `relation_embedder`, minimizing the amount of work it has to do.
Even in a slightly worse scenario where they don't all end up in the same batch, if the interval is big
 enough so that it's likely that a significant proportion af them are in the same batch, this drasticxally reduces the 
 amount of messages sent to the `relation_embedder`.

### Relation embedder
The `relation_embedder` receives `collectionPath`s as input from the `batcher` instead of work ids. 
It is unchanged in every other aspect.

## Potential Issues

One issue is that we introduce a delay in processing updates to works in an archive. We can tweak the `batcher` 
interval to make this less of an issue, but the more we decrease the delay in the `batcher` the more we make the 
`relation_embedder` do unnecessary work, which in turn increases delay again.
One possible solution could be to make the interval duration dynamic, so that it's big while there are many messages
 in flight, but it's small if there are not many messages. This could decrease latency for updates, 
 while retaining efficiency advantages during a reindex.
 
Also, it's worth mentioning that currently, only 1/6th of works are in an archive, so the majority of 
works will probably be sped up by the fact that they can bypass
 the relation_embedder stage entirely. 
